{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL5Aai5kN5c_"
      },
      "source": [
        "# **Hugging Face**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Auther: Tassawar Abbas\\\n",
        "Email: abbas829@gmail.com\\\n",
        "Kaggle: https://www.kaggle.com/abbas829\\\n",
        "gitub: https://www.github.com/abbas829"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Introducing Hugging Face: Revolutionizing AI Development\n",
        "\n",
        "In recent years, Hugging Face has emerged as a game-changer in the field of artificial intelligence (AI) development. With its user-friendly interfaces, extensive libraries, and state-of-the-art models, Hugging Face has democratized AI and accelerated innovation in natural language processing (NLP) and beyond. Let's delve into what Hugging Face is all about and how it's transforming the AI landscape.\n",
        "\n",
        "## What is Hugging Face?\n",
        "\n",
        "Hugging Face is an AI research organization and a leading provider of open-source libraries and tools for NLP. Founded in 2016, Hugging Face has quickly gained popularity among developers, researchers, and industry professionals for its contributions to the AI community. The company is committed to advancing AI research and democratizing access to cutting-edge models and technologies.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "### 1. Transformers Library\n",
        "\n",
        "At the heart of Hugging Face is the Transformers library, a comprehensive collection of pre-trained models for NLP tasks such as text classification, sentiment analysis, machine translation, and more. These models, ranging from small to large architectures, are trained on massive datasets and fine-tuned for various downstream tasks, allowing developers to leverage state-of-the-art performance with minimal effort.\n",
        "\n",
        "### 2. Model Hub\n",
        "\n",
        "Hugging Face provides a centralized Model Hub, where users can discover, share, and deploy pre-trained models for a wide range of NLP tasks. The Model Hub hosts thousands of models contributed by the community, including models developed by Hugging Face and models from leading research institutions and industry partners. This extensive repository enables researchers and practitioners to access a diverse array of models and experiment with different architectures and capabilities.\n",
        "\n",
        "### 3. ðŸ¤— Accelerated Inference\n",
        "\n",
        "Hugging Face offers ðŸ¤— Accelerated Inference, a cloud-based service that enables fast and scalable deployment of AI models in production. With ðŸ¤— Accelerated Inference, users can seamlessly deploy models for inference, monitor performance, and scale resources as needed, streamlining the deployment process and reducing time-to-market for AI applications.\n",
        "\n",
        "## Getting Started with Hugging Face\n",
        "\n",
        "Getting started with Hugging Face is easy and intuitive. Whether you're a seasoned AI researcher or a novice developer, Hugging Face provides comprehensive documentation, tutorials, and examples to help you get up and running quickly. Here's a simple example of using Hugging Face's Transformers library to perform sentiment analysis:\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze sentiment of text\n",
        "result = classifier(\"I love Hugging Face!\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Brief Overview of Transformers\n",
        "\n",
        "Transformers represent a groundbreaking architecture in the field of natural language processing (NLP) and have significantly advanced the state-of-the-art in various NLP tasks. Originally introduced by Vaswani et al. in the paper \"Attention Is All You Need\" in 2017, transformers have since become the foundation for many cutting-edge NLP models, including BERT, GPT, RoBERTa, and many others.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "### Self-Attention Mechanism\n",
        "\n",
        "Transformers rely on a self-attention mechanism, allowing them to weigh the importance of different words in a sentence based on their contextual relevance. This mechanism enables transformers to capture long-range dependencies and relationships within the input text more effectively compared to traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
        "\n",
        "### Multi-Head Attention\n",
        "\n",
        "In addition to self-attention, transformers employ multi-head attention, which enhances the model's capacity to focus on different parts of the input text simultaneously. By attending to multiple representation subspaces, multi-head attention enables transformers to capture diverse linguistic patterns and semantic information, leading to more robust and expressive representations.\n",
        "\n",
        "### Positional Encoding\n",
        "\n",
        "To preserve the sequential order of words in a sentence, transformers incorporate positional encoding, which provides the model with information about the relative positions of words within the input sequence. Positional encoding enables transformers to account for word order and sequential relationships, essential for tasks such as language modeling and sequence generation.\n",
        "\n",
        "### Feedforward Neural Networks\n",
        "\n",
        "Transformers include feedforward neural networks (FFNs) as part of their architecture, allowing them to capture complex non-linear relationships and perform various transformations on input embeddings. FFNs consist of multiple layers of fully connected neural units, enabling transformers to learn hierarchical representations and extract high-level features from the input text.\n",
        "\n",
        "## Applications\n",
        "\n",
        "Transformers have been applied to a wide range of NLP tasks, including:\n",
        "\n",
        "- Text classification\n",
        "- Sentiment analysis\n",
        "- Named entity recognition\n",
        "- Machine translation\n",
        "- Question answering\n",
        "- Summarization\n",
        "- Text generation\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Transformers have revolutionized the field of NLP and continue to drive advancements in AI research and applications. With their ability to model long-range dependencies, capture contextual information, and generate coherent text, transformers have become indispensable tools for natural language understanding and generation tasks, paving the way for more sophisticated and intelligent AI systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "paJV809SLO4W"
      },
      "outputs": [],
      "source": [
        "# Installing the required modules\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kBCcCZVhLZIZ"
      },
      "outputs": [],
      "source": [
        "# Installing the required modules\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY0gQaB0juMp"
      },
      "source": [
        "## **Text Classification**\n",
        "\n",
        "We can classify the text using the Hugging face prebuit pipelines\n",
        "we will extensively use **pipline()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Brief Overview of Pipelines\n",
        "\n",
        "Pipelines in the context of machine learning refer to a sequence of data processing components that are chained together to automate a workflow. Pipelines streamline the process of model development, deployment, and evaluation by encapsulating multiple steps into a single entity. In the context of libraries such as scikit-learn and Hugging Face's Transformers, pipelines are commonly used to chain together data preprocessing, model training, and inference steps.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "### Modularity\n",
        "\n",
        "Pipelines allow for the modularization of machine learning workflows, enabling developers to break down complex tasks into smaller, more manageable components. Each component in the pipeline performs a specific task, such as data preprocessing, feature extraction, model training, or inference, making it easier to debug, maintain, and scale the workflow.\n",
        "\n",
        "### Automation\n",
        "\n",
        "By automating the sequence of steps required for model development and deployment, pipelines help reduce manual intervention and minimize human error. Once configured, pipelines can execute the entire workflow with a single command, from data preprocessing to model evaluation, speeding up the development cycle and improving productivity.\n",
        "\n",
        "### Flexibility\n",
        "\n",
        "Pipelines provide flexibility in designing and customizing machine learning workflows to suit specific requirements and use cases. Developers can easily swap out components or add new ones to adapt the pipeline to changing data sources, model architectures, or business objectives, without having to restructure the entire workflow.\n",
        "\n",
        "### Reproducibility\n",
        "\n",
        "Pipelines promote reproducibility in machine learning experiments by encapsulating the entire workflow, including data preprocessing, model training, and evaluation, into a single entity. This allows researchers and practitioners to reproduce experimental results with ease and compare different models or configurations systematically.\n",
        "\n",
        "## Applications\n",
        "\n",
        "Pipelines find applications across various domains and use cases in machine learning, including:\n",
        "\n",
        "- Text classification\n",
        "- Sentiment analysis\n",
        "- Image classification\n",
        "- Object detection\n",
        "- Time series forecasting\n",
        "- Recommender systems\n",
        "- Natural language processing\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Pipelines play a crucial role in automating and streamlining machine learning workflows, from data preprocessing to model deployment. By encapsulating multiple steps into a single entity, pipelines simplify the development process, improve productivity, and enhance reproducibility in machine learning experiments, making them indispensable tools for researchers, practitioners, and businesses alike.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBxquZRGkbER"
      },
      "source": [
        "## **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CDpEI0rJKpQT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\abbas\\miniconda3\\envs\\hugging_face\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abbas\\miniconda3\\envs\\hugging_face\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abbas\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\abbas\\miniconda3\\envs\\hugging_face\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9998028874397278}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\")\n",
        "pipe(\"This movie is very boring\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ANV_G8Gp9MHx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abbas\\miniconda3\\envs\\hugging_face\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abbas\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEUTRAL', 'score': 0.7139405012130737}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(model=\"roberta-large-mnli\")\n",
        "pipe(\"I do not like this movie\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKl7fqHCmsl5"
      },
      "source": [
        "**We can also pass the string in the form of list**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3L3_MkqVbUHZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998743534088135},\n",
              " {'label': 'NEGATIVE', 'score': 0.9996669292449951}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pipeline for text classification\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\")\n",
        "pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZWHaYdnm-9x"
      },
      "source": [
        "## **Text Summrization**\n",
        "\n",
        "we can summarize the text using the hugging face pipline\n",
        "for that we need to pass the pipeline the parameter as \"summarization\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0OZyfkDb5FGM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start by providing your text input. It could be a sentence or a paragraph.\n",
            " Tokenization: The input is tokenized, which means breaking it down into smaller units like words or subwords.\n",
            " Tokens are the building blocks for NLP models\n",
            ".Model: The tokenized input is passed through a pre-trained NLP model. \n",
            " Hugging Face offers a wide range of models for different NLP tasks, such as sentiment analysis,\n",
            "  question answering, and text generation.Prediction/Output: The model processes the tokenized input and generates \n",
            " a prediction or output specific to the task. For example, if it's sentiment analysis, \n",
            " it could predict whether the input is positive or negative\n"
          ]
        }
      ],
      "source": [
        "# text to be summarized\n",
        "input_text = \"Start by providing your text input. It could be a sentence or a paragraph.\\n Tokenization: The input is tokenized, which means breaking it down into smaller units like words or subwords.\\n Tokens are the building blocks for NLP models\\n.Model: The tokenized input is passed through a pre-trained NLP model. \\n Hugging Face offers a wide range of models for different NLP tasks, such as sentiment analysis,\\n  question answering, and text generation.Prediction/Output: The model processes the tokenized input and generates \\n a prediction or output specific to the task. For example, if it's sentiment analysis, \\n it could predict whether the input is positive or negative\"\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PQBF3E3ag2xZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "c:\\Users\\abbas\\miniconda3\\envs\\hugging_face\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abbas\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'the input is tokenized, which means breaking it down into smaller units like words or subwords . the model processes the tokenized input'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use bart in pytorch\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\"Text Input: Start by providing your text input. It could be a sentence or a paragraph.Tokenization: The input is tokenized, which means breaking it down into smaller units like words or subwords. Tokens are the building blocks for NLP models.Model: The tokenized input is passed through a pre-trained NLP model. Hugging Face offers a wide range of models for different NLP tasks, such as sentiment analysis, question answering, and text generation.Prediction/Output: The model processes the tokenized input and generates a prediction or output specific to the task. For example, if it's sentiment analysis, it could predict whether the input is positive or negative.\", min_length=5, max_length=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMYDAtZNnwai"
      },
      "source": [
        "## **Name Entity Relation**\n",
        "\n",
        "Named Entity Recognition is a natural language processing (NLP) task\n",
        "\n",
        "that involves identifying and classifying named entities in text into\n",
        "\n",
        "predefined categories such as person names, organizations, locations, dates, and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L11EYoZ0lrXD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "c:\\Users\\abbas\\miniconda3\\envs\\hugging_face\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abbas\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'entity': 'I-PER', 'score': 0.99899393, 'index': 4, 'word': 'Ahmad', 'start': 11, 'end': 16}, {'entity': 'I-LOC', 'score': 0.9998221, 'index': 10, 'word': 'Pakistan', 'start': 35, 'end': 43}]\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"ner\")\n",
        "example = \"My name is Ahmad and i am going to Pakistan\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2kLc7zHpS6a"
      },
      "source": [
        "## **Image Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5VmiyYdGfW7"
      },
      "source": [
        "To use this pipline download the image in PNG format.\n",
        "\n",
        "Click on the folder icon on the left side panel.\n",
        "\n",
        "Upload the image and copy its path in the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O_2vJANCZuYY"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image.save(\"astronaut_rides_horse.png\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
